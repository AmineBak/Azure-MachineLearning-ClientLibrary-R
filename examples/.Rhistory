}
if (missing(requestURL)) {
stop("Need to specify request URL")
}
if (missing(columnNames)) {
stop("Need to specify column names")
}
if(missing(globalParam)) {
globalParam = ""
}
#store arguments as mega list of lists
valuesList <- lapply(X=list(...), function(x) x)
#make api call with components of payload
results <- callDTAPI(api_key, requestURL, columnNames, valuesList,  globalParam, retryDelay)
results <- jsonlite::fromJSON(results)
resultValues = results$Results$output1$value
# Previous lines were commented out, would not return correctly if there were multiple return values
#resultDF <- data.frame(resultList[,(ncol(resultList))])
#colnames(resultDF) = "Scored probabilities"
resultDF <- data.frame(resultValues$Values)
colnames(resultDF) <- resultValues$ColumnNames
return(resultDF)
}
#############################################################
#' @title Consume Lists
#' @description
#' This function takes in an API key, the request URL (OData Endpoint Address), the column names and multiple requests
#' It scores the experiment with the requests stored in a list of lists, and sends it to the server in the appropriate format.
#' It then obtains a response from Azure Machine Learning Studio and returns a response to the user. It returns the output column(s) along with the scored probablities!
#' @param string The api key must be entered as the first parameter
#' @param string requestURL must be entered as the third parameter
#' @param list columnNames - column Names
#' @param ... each parameter must be a request in the format of a list that contains a row of values corresponding to the column names provided
#' @param string globalParam - global parameters, default value is ""
#' @param int retryDelay the time in seconds to delay before retrying in case of a server error, default value of 0.3 seconds
#' @return results in a list of lists, with the scored probability at the end of each list
#' @examples
#' # First, consume as lists
#' # First, consume with inputs as a list
#' response <- consumeLists(endpoints[[1]]["PrimaryKey"], paste(endpoints[[1]]["ApiLocation"], "/execute?api-version=2.0&details=true",sep=""), list("Pclass", "Sex", "Age", "SibSp", "Parch", "Fare"), list(1, "male", 20, 2, 0, 8.50), list(1, "female", 20, 1, 0, 8.50))
#############################################################
consumeLists <- function(api_key, requestURL, ..., globalParam = setNames(list(), character(0)), retryDelay = 0.3) {
if (missing(api_key)) {
stop("Need to specify API key")
}
if (missing(requestURL)) {
stop("Need to specify request URL")
}
if(missing(globalParam)) {
globalParam = setNames(list(), character(0))
}
df <- data.frame(stringsAsFactors=FALSE)
#store arguments as mega list of lists
keyvalues <- list(...)
#make api call with components of payload
temp <- callAPI(api_key, requestURL, keyvalues,  globalParam, retryDelay)
resultStored <- jsonlite::fromJSON(temp)
resultList = resultStored$Results$output1
resultDF <- data.frame(resultList[,(ncol(resultList))])
df <- rbind(df,resultDF)
colnames(df) <- "Scored probabilities"
return(df)
}
#############################################################
#' @title Consume Data Frame
#' @description
#' This function takes in an API key, the request URL (OData Endpoint Address), the column names and multiple requests
#' It scores the experiment with the requests stored in a list of lists, and sends it to the server in the appropriate format.
#' It then obtains a response from Azure Machine Learning Studio and returns a response to the user. It returns the output column(s) along with the scored probablities!
#' @param string The api key must be entered as the first parameter
#' @param string requestURL must be entered as the second parameter
#' @param string valuesDF - The name of the data frame that is being scored
#' @param string globalParam - global parameters, default value is ""
#' @param int batchSize of each batch, which is optional, but 100 by default
#' @param int retryDelay the time in seconds to delay before retrying in case of a server error, default value of 0.3 seconds
#' @return results in a list of lists, with the scored probability at the end of each list
#' @examples
#' # Using the titanic as the example code and model
#' # consume with inputs as dataframe
#' # creating test data.frame
#' demoDF <- data.frame("Pclass"=c(1,2,1), "Sex"=c("male","female","male"), "Age"=c(8,20, 30), "Parch"=c(1,1,1), "SibSp"=c(1,3,1), "Fare"=c(10,7.5, 9))
#' responseDF <- consumeDataframe(TitanicService[[2]][[1]]$PrimaryKey, paste(TitanicService[[2]][[1]]$ApiLocation,"/execute?api-version=2.0&details=true",sep=""), demoDF)
#############################################################
consumeDataframe <- function(api_key, requestURL, valuesDF, globalParam=setNames(list(), character(0)), batchSize = 250, retryDelay = 0.3) {
if (missing(api_key)) {
stop("Need to specify API key")
}
if (missing(requestURL)) {
stop("Need to specify request URL")
}
if (missing(valuesDF)) {
stop("Need to specify dataframe to be scored")
}
#format as matrix and parse column by column
columnNames = colnames(valuesDF)
matrixdf <- as.matrix(valuesDF)
rownames(matrixdf) <- NULL
colnames(matrixdf) <- NULL
matrixdf <- lapply(seq_len(nrow(matrixdf)), function(row) matrixdf[row,])
values = matrixdf
df <- data.frame(stringsAsFactors=FALSE)
valuebatch = list()
counter = 1
#process in batches and make API calls in batches
for(i in 1:(length(values))) {
valuebatch[length(valuebatch) + 1] = values[i]
if(counter == batchSize || i == (length(values))) {
temp <- callDTAPI(api_key, requestURL, columnNames, valuebatch, globalParam, retryDelay)
resultStored <- jsonlite::fromJSON(temp)
resultList = resultStored$Results$output1$value$Values
resultDF <- data.frame(resultList[,(ncol(resultList))])
#      print(resultDF)
#      print(is.data.frame(resultDF))
if(length(df) != 0 && length(resultDF) != 0) {
names(df) <- names(resultDF)
}
df <- rbind(df,resultDF)
colnames(df) <- "Scored probabilities"
#      print("passed")
sprintf("%i out of %i processed", i, length(values))
valuebatch = list()
counter = 0
}
counter = counter + 1
}
colnames(df) <- "Scored probabilities"
return(df)
#   resultStored <- jsonlite::fromJSON(resultStored)
#   resultDF <- data.frame(matrix(resultStored$Results$output1$value$Values))
#   colnames(resultDF) <- resultStored$Results$output1$value$ColumnNames
#   return(resultDF)
}
#############################################################
#' @title HELPER FUNCTION: Call DT API
#' @description
#' This function is a helper that takes in an API key, request URL, column names of the data, request in the data table format (in a lists of lists), global parameters of a web service, and delay time before retrying a call in case of a server error.
#' It then obtains a response from Azure Machine Learning Studio in the JSON format and returns a response to the consumption functions that call it.
#' @param string apiKey
#' @param string requestUrl entered as a string or discovered through the discover schema method
#' @param list columnNames - column names entered as a list or discovered through the discover schema method
#' @param list requestList
#' @param list globalParam - global parameters entered as a list, default value is an empty list
#' @param int retryDelay the time in seconds to delay before retrying in case of a server error
#' @return result from the DT API Call
#############################################################
callDTAPI <- function(api_key, requestURL, columnNames, values,  globalParam, retryDelay) {
httpStatus = 0
tries = 0
# limit number of API calls to 3
for(i in 1:3) {
#make api calls and prepare payload if httpStatus indicates server error or if
if(tries == 0 || httpStatus >= 500) {
if(httpStatus >= 500) {
#delay by fixed or specified time if server error
print(paste("The request failed with status code:", httpStatus, sep=" "))
print("headers:")
print(headers)
print(sprintf("%s %f %s", "Retrying in ",retryDelay," seconds"))
Sys.sleep(retryDelay)
}
tries = tries + 1
#construct request payload and load RCurl functions
options(RCurlOptions = list(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl")))
h = RCurl::basicTextGatherer()
hdr = RCurl::basicHeaderGatherer()
req = list(
Inputs = list(
"input1" = list(
"ColumnNames" = columnNames,
"Values" = values
)
)
,GlobalParameters = globalParam
)
body = enc2utf8((rjson::toJSON(req)))
#make call to API after constructing request payload
authz_hdr = paste('Bearer', api_key, sep=' ')
h$reset()
RCurl::curlPerform(url = requestURL,
httpheader=c('Content-Type' = "application/json", 'Authorization' = authz_hdr),
postfields=body,
writefunction = h$update,
headerfunction = hdr$update,
verbose = TRUE
#                 Parameters below are needed if using test environment, but should not be included for security reasons
,ssl.verifypeer=FALSE,
ssl.verifyhost = FALSE
)
headers = hdr$value()
httpStatus = headers["status"]
result = h$value()
formatresult = result
#      formatresult <- jsonlite::toJSON(jsonlite::fromJSON(result), pretty = TRUE)
}
#return if successful
if(httpStatus == 200) {
return(formatresult)
}
#if user error, print and return error details
else if ((httpStatus>= 400) && (500 > httpStatus))
{
print(paste("The request failed with status code:", httpStatus, sep=" "))
print("headers:")
print(headers)
print(h$value())
break
}
}
return(formatresult)
}
#############################################################
#' @title HELPER FUNCTION: Call API
#' @export internal
#' @description
#' This function is a helper that takes in an API key, request URL, request in the key value format (in a lists of lists), global parameters of a web service, and delay time before retrying a call in case of a server error.
#' It then obtains a response from Azure Machine Learning Studio in the JSON format and returns a response to the consumption functions that call it
#' @param string apiKey
#' @param string requestUrl entered as a string or discovered through the discover schema method
#' @param list columnNames - column names entered as a list or discovered through the discover schema method
#' @param requestList
#' @param list globalParam - global parameters entered as a list, default value is an empty list
#' @param retryDelay the time in seconds to delay before retrying in case of a server error, default value is 0.3 seconds
#' @return result of the API call
#############################################################
callAPI <- function(api_key, requestURL, keyvalues,  globalParam, retryDelay) {
httpStatus = 0
tries = 0
# limit number of API calls to 3
for(i in 1:3) {
#make api calls and prepare payload if httpStatus indicates server error or if
if(tries == 0 || httpStatus >= 500) {
if(httpStatus >= 500) {
#delay by fixed or specified time if server error
print(paste("The request failed with status code:", httpStatus, sep=" "))
print("headers:")
print(headers)
print(sprintf("%s %f %s", "Retrying in ",retryDelay," seconds"))
Sys.sleep(retryDelay)
}
tries = tries + 1
#construct request payload and load RCurl functions
options(RCurlOptions = list(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl")))
h = RCurl::basicTextGatherer()
hdr = RCurl::basicHeaderGatherer()
req = list(
Inputs = list(
input1 = keyvalues
)
,GlobalParameters = globalParam
)
body = enc2utf8((rjson::toJSON(req)))
print(body)
#make call to API after constructing request payload
authz_hdr = paste('Bearer', api_key, sep=' ')
h$reset()
RCurl::curlPerform(url = requestURL,
httpheader=c('Content-Type' = "application/json", 'Authorization' = authz_hdr),
postfields=body,
writefunction = h$update,
headerfunction = hdr$update,
verbose = TRUE
#                 Parameters below are needed if using test environment, but should not be included for security reasons
,ssl.verifypeer=FALSE,
ssl.verifyhost = FALSE
)
headers = hdr$value()
httpStatus = headers["status"]
result = h$value()
formatresult = result
}
#return if successful
if(httpStatus == 200) {
return(formatresult)
}
#if user error, print and return error details
else if ((httpStatus>= 400) && (500 > httpStatus))
{
print(paste("The request failed with status code:", httpStatus, sep=" "))
print("headers:")
print(headers)
break
}
}
return(formatresult)
}
#############################################################
#' @title HELPER FUNCTION: Discover Schema
#' @export internal
#' @description
#' This function returns to the user a list of optional functions the user can perform and the returns the schema of their requested workspace.
#' @param string wkID - workspace ID retrieved from your AzureML account
#' @param string token - the authentication token retrieved from your AzureML account
#' @param string schemes - default is https
#' @param string host - default is "requestresponse001.cloudapp.net:443"
#' @param string api_version - default api version is 2.0
#' @return The schema of the call the user made
#############################################################
discoverSchema <- function(wkID, token, schemes = "https", host = "requestresponse001.cloudapp.net:443", api_version = "2.0") {
# swagger document:
# schemes://hostbasepath/"swagger.json?api-version=2.0"
swaggerURL = paste(schemes,"://", host, "/workspaces/", wkID, "/services/", token,"/swagger.json?api-version=",api_version, sep = "")
httr::set_config(config(ssl_VERIFYHOST=FALSE,ssl_verifyPEER=FALSE), override=TRUE)
resp <- httr::GET(swaggerURL)
swagger <- httr::content(resp)
# condensed three steps into one line: Access JSON and then use rjson and json lite in order to structure it as a layered json object
inputschema = jsonlite::toJSON(jsonlite::fromJSON((rjson::toJSON(swagger$definitions$ExecutionInputs))), pretty = TRUE)
inputexample <- jsonlite::toJSON(jsonlite::fromJSON((rjson::toJSON(swagger$definitions$ExecutionRequest$example))), pretty = TRUE)
#find the path where operationId is execute
foundExec = FALSE
pathno = 0
foundpathindex= -1
for(execpath in swagger$paths) {
pathno = pathno + 1
for(operationpath in execpath) {
for(operation in operationpath) {
for(charac in operation) {
if(charac[1] == "execute")
{
foundExec = TRUE
foundpathindex = pathno
break
}
}
}
}
}
executepath = names(swagger$paths)[[foundpathindex]]
httpMethod = toupper(names(swagger$paths[[2]]))
# requestURL:
#   "https://requestresponse001.cloudapp.net:443/workspaces/7e8f135f31274b7eac419bd056875c03/services/a5b003e52c924d16a2e38ade45dd0154/execute?api-version=2.0&format=swagger"
#   schemes://hostbasepath(path where operationId="execute")
requestURL = paste(schemes,"://", host, "/workspaces/", wkID, "/services/", token, executepath, sep = "")
httpRequest = paste(httpMethod,requestURL)
#tell user what they can do
if(foundExec) {
consumefile = paste("consumeFile(api_key, requestURL, dataframe)")
consumedf = paste("consumeDataframe(api_key, requestURL, valuesDF)")
consumelists = paste("consumeLists(api_key, requestURL, ...)")
consumedt = paste("consumeFile(api_key, requestURL, columnNames, ...)")
cat("Sample functions to execute the web service and get a response synchronously:","\n", consumefile,"\n", consumedf,"\n", consumelists,"\n", consumedt,"\n","\n")
}
return (list("Request URL:" = requestURL, "Sample input:" = inputexample, "Input schema:" = inputschema))
}
setwd("C://Users/t-alewa/Documents/Azure-MachineLearning-ClientLibrary-R/examples")
#setwd("C://Users/t-ritra/Documents/Github/Azure-MachineLearning-ClientLibrary-R/test")
wsID = "3612640f27234eb7b2b91ac62e8b4a40"
wsAuth = "abcbe14a958a40978f93aa0e0e71f5be"
library("e1071")
dataset <- read.csv(file="iris.csv")
# Train the model
model <- naiveBayes(as.factor(Species) ~., dataset)
predictClass <- function(sepalLength, sepalWidth, petalLength, petalWidth) {
predictDF <- predict(model, data.frame("sepalLength" = sepalLength, "sepalWidth" = sepalWidth, "petalLength" = petalLength, "petalWidth"=petalWidth), type="raw")
return(colnames(predictDF)[apply(predictDF, 1, which.max)])
}
irisService <- publishWebService("predictClass", "irisService7-21", list("sepalLength"="float", "sepalWidth"="float", "petalLength"="float", "petalWidth"="float"), list("class"="int"), wsID, wsAuth)
endpoints <- irisService[[2]]
response <- consumeDataTable(endpoints[[1]]["PrimaryKey"], endpoints[[1]]["ApiLocation"], list("sepalLength", "sepalWidth", "petalLength", "petalWidth"), list(5, 5, 4, 3), list(4.5, 6.5, 4.5, 2))
response <- consumeDataTable(endpoints[[1]]$PrimaryKey, endpoints[[1]]$ApiLocation, list("sepalLength", "sepalWidth", "petalLength", "petalWidth"), list(5, 5, 4, 3), list(4.5, 6.5, 4.5, 2))
?naiveBayes
class(naiveBayes)
library(codetools)
findGlobals(predictClass)
model
typeof(model)
class(model)
environment(class(model))
environment(get(class(model))
)
getNamespaceName(environment(get(class(model))))
packDependencies <- function(functionName) {
# Recursive step for package packaging
recurPkg <- function(pkgName, pkgList) {
# if the package isn't already in the list
if (!(pkgName %in% pkgList)) {
# add it
pkgList <- c(pkgName, pkgList)
# if the package is available on a repo
if (pkgName %in% row.names(available.packages())) {
# iterate through the dependencies and check if need to add them
for (pkg in strsplit(available.packages()[pkgName, "Depends"], split=", ")[[1]]) {
# filter out duplicates, R version dependencies, and base packages
if (!(pkg %in% pkgList) && !(grepl("R \\((.*)\\)", pkg)) && (pkg %in% row.names(available.packages()))) {
# recursively call recurPkg
pkgList <- recurPkg(pkg, pkgList)
}
}
# iterate through imports
for (pkg in strsplit(available.packages()[pkgName, "Imports"], split=", ")[[1]]) {
# filter out duplicates, R version dependencies, and base packages
if (!(pkg %in% pkgList) && !(grepl("R \\((.*)\\)", pkg)) && (pkg %in% row.names(available.packages()))) {
# recursively call recurPkg
pkgList <- recurPkg(pkg, pkgList)
}
}
}
}
# return updated list of packages
return(pkgList)
}
# Recursive step for object packaging
# NOTE: will not work if the user function specifies the names directly, e.g. won't find rjson::toJSON
# from findGlobals man page: "R semantics only allow variables that might be local to be identified"
recurDep <- function(objName, depList, pkgList) {
# findGlobals() gets all external dependencies
# Iterate over them
for (obj in codetools::findGlobals(get(objName))) {
name = get(obj)
# filter out primitives and duplicates
if (is.primitive(name) || (obj %in% names(depList))) {
next
}
# non-function object dependencies
else if (!is.function(name)) {
depList[[obj]] <- name
# Use the object's class to find package dependencies
objClass <- class(name)
# iterate through the class vector looking for packages
for (class in objClass) {
tryCatch({
# get the name of the package the class belongs to
nameEnv <- environment(get(class))
# filter out basic environment
if (!(identical(nameEnv, NULL)) && !(identical(nameEnv, .BaseNamespaceEnv))) {
pkgList <- recurPkg(paste(getNamespaceName(nameEnv)), pkgList)
}
# if unable to find package, continue
}, error = function(e) {
sprintf("%s not found", obj)
})
}
}
# user defined functions
else if (identical(environment(name), globalenv())) {
depList[[obj]] <- name
results <- recurDep(obj, depList, pkgList)
depList <- results$dependencies
pkgList <- results$packages
}
# functions from packages
else if (paste(getNamespaceName(environment(name))) != "base") {
pkgList <- recurPkg(paste(getNamespaceName(environment(name))), pkgList)
}
}
return(list("dependencies"=depList, "packages"=pkgList))
}
# call recurDep on the desired function and with empty lists
results <- recurDep(functionName, list(), list())
dependencies <- results$dependencies
packages <- results$packages
# save current path to restore later
start = getwd()
# go to package library, doing this to prevent zipping entire path to package
toPack <- packages
toZip = vector()
for (i in 1:length(.libPaths())) {
setwd(.libPaths()[i])
# try to find the package in the directory and zip it
for (pkg in toPack) {
if (file.exists(pkg)) {
# save it to original directory
zip(paste(start, paste(pkg, "zip", sep="."), sep="/"), pkg)
toZip <- c(toZip, paste(pkg, "zip", sep="."))
# remove the package from the list of packages to pack
toPack <- toPack[toPack != pkg]
}
}
# if done packing, break
if (length(toPack) == 0) {
break
}
}
# go back to where the user started
setwd(start)
# make sure that all packages were found
if (length(toPack) > 0) {
stop("Error: unable to locate one or more packages. Please make sure the packages used are in at least one of the library paths.")
}
# generate a GUID to act as a file name to store packages, R data
guid = gsub("-", "", uuid::UUIDgenerate(use.time=TRUE))
# dump objects, functions, etc. into .rdta file
if (length(dependencies) > 0) {
# maybe can save directly as a .zip and skip the zip() call?
save(dependencies, file=guid)
toZip <- c(toZip, guid)
}
# zip up everything
if (length(toZip) > 0) {
zip(zipfile=guid, files=toZip)
zipEnc <- base64enc::base64encode(paste(guid, ".zip", sep=""))
# delete the packages
for (pkg in packages) {
file.remove(paste(pkg, "zip", sep="."))
}
# delete the dependency rdta file
if (length(dependencies) > 0) {
file.remove(guid)
file.remove(paste(guid,"zip",sep="."))
}
# return the encoded zip as a string
return(list(guid, zipEnc))
}
# if nothing was zipped, return empty string to indicate
# returning two things because unable to return variable amounts
else {
return(list(guid, ""))
}
}
irisService <- publishWebService("predictClass", "irisService7-21", list("sepalLength"="float", "sepalWidth"="float", "petalLength"="float", "petalWidth"="float"), list("class"="int"), wsID, wsAuth)
endpoints <- irisService[[2]]
response <- consumeDataTable(endpoints[[1]]$PrimaryKey, endpoints[[1]]$ApiLocation, list("sepalLength", "sepalWidth", "petalLength", "petalWidth"), list(5, 5, 4, 3), list(4.5, 6.5, 4.5, 2))
response
