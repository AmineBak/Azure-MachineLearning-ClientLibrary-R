if (missing(requestURL)) {
stop("Need to specify request URL")
}
#read file into dataframe, convert into dataframe
valuesDF = read.csv(infileName,check.names=FALSE)
df <- data.frame(stringsAsFactors=FALSE)
valuebatch = data.frame(stringsAsFactors=FALSE)
counter = 1
lastproc = 0
#process in batches and make API calls in batches
for(i in 1:(nrow(valuesDF))) {
if(counter == batchSize || i == (nrow(valuesDF))) {
resultDF = data.frame(stringsAsFactors=FALSE)
valuebatch = valuesDF[(lastproc+1):i,]
keyvalues = rjson::fromJSON((df2json::df2json(valuebatch)))
temp <- callAPI(api_key, requestURL, keyvalues, globalParam, retryDelay)
lastproc = i
resultStored <- jsonlite::fromJSON(temp)
resultList = resultStored$Results$output1
resultDF <- data.frame(resultList[,(ncol(resultList))])
if(length(df) != 0 && length(resultDF) != 0) {
names(df) <- names(resultDF)
}
df <- rbind(df,resultDF)
print(sprintf("%i %s %i %s", i,"out of",nrow(valuesDF),"processed"))
valuebatch = data.frame(stringsAsFactors=FALSE)
counter = 0
}
counter = counter + 1
}
colnames(df) <- "Scored probabilities"
fileConn <-file(outfileName,"w")
write.csv(df, fileConn)
close(fileConn)
return (df)
}
#' This function takes in an API key, the request URL (OData Endpoint Address), the column names and multiple requests
#' It scores the experiment with the requests stored in a list of lists, and sends it to the server in the appropriate format.
#' It then obtains a response from Azure Machine Learning Studio and returns a response to the user. It returns the output column(s) along with the scored probablities!
#' @param api key must be entered as the first parameter, and must be a string
#' @param requestURL must be entered as the third parameter, and must be a string
#' @param columnNames entered as a list
#' @param ... each parameter must be a request in the format of a list that contains a row of values corresponsing to the column names provided
#' @param globalParam global parameters entered as a string, default value is ""
#' @param retryDelay the time in seconds to delay before retrying in case of a server error, default value of 0.3 seconds
#' @return results in a list of lists, with the scored probability at the end of each list
consumeDataTable <- function(api_key, requestURL, columnNames, ..., globalParam="", retryDelay = 0.3) {
if (missing(api_key)) {
stop("Need to specify API key")
}
if (missing(requestURL)) {
stop("Need to specify request URL")
}
if (missing(columnNames)) {
stop("Need to specify column names")
}
if(missing(globalParam)) {
globalParam = ""
}
#store arguments as mega list of lists
valuesList <- lapply(X=list(...), function(x) x)
#make api call with components of payload
results <- callDTAPI(api_key, requestURL, columnNames, valuesList,  globalParam, retryDelay)
results <- jsonlite::fromJSON(results)
resultValues = results$Results$output1$value
# Previous lines were commented out, would not return correctly if there were multiple return values
#resultDF <- data.frame(resultList[,(ncol(resultList))])
#colnames(resultDF) = "Scored probabilities"
resultDF <- data.frame(resultValues$Values)
colnames(resultDF) <- resultValues$ColumnNames
return(resultDF)
}
#' This function takes in an API key, the request URL (OData Endpoint Address), the column names and multiple requests
#' It scores the experiment with the requests stored in a list of lists, and sends it to the server in the appropriate format.
#' It then obtains a response from Azure Machine Learning Studio and returns a response to the user. It returns the output column(s) along with the scored probablities!
#' @param api key must be entered as the first parameter, and must be a string
#' @param requestURL must be entered as the third parameter, and must be a string
#' @param columnNames entered as a list
#' @param ... each parameter must be a request in the format of a list that contains a row of values corresponding to the column names provided
#' @param globalParam global parameters entered as a string, default value is ""
#' @param retryDelay the time in seconds to delay before retrying in case of a server error, default value of 0.3 seconds
#' @return results in a list of lists, with the scored probability at the end of each list
consumeLists <- function(api_key, requestURL, ..., globalParam = setNames(list(), character(0)), retryDelay = 0.3) {
if (missing(api_key)) {
stop("Need to specify API key")
}
if (missing(requestURL)) {
stop("Need to specify request URL")
}
if(missing(globalParam)) {
globalParam = setNames(list(), character(0))
}
df <- data.frame(stringsAsFactors=FALSE)
#store arguments as mega list of lists
keyvalues <- list(...)
#make api call with components of payload
temp <- callAPI(api_key, requestURL, keyvalues,  globalParam, retryDelay)
resultStored <- jsonlite::fromJSON(temp)
resultList = resultStored$Results$output1
resultDF <- data.frame(resultList[,(ncol(resultList))])
df <- rbind(df,resultDF)
colnames(df) <- "Scored probabilities"
return(df)
}
#' This function takes in an API key, the request URL (OData Endpoint Address), the column names and multiple requests
#' It scores the experiment with the requests stored in a list of lists, and sends it to the server in the appropriate format.
#' It then obtains a response from Azure Machine Learning Studio and returns a response to the user. It returns the output column(s) along with the scored probablities!
#' @param api key must be entered as the first parameter, and must be a string
#' @param requestURL must be entered as the third parameter, and must be a string
#' @param valuesDF the name of the data frame that is being scored
#' @param globalParam global parameters entered as a string, default value is ""
#' @param batchSize of each batch, which is optional, but 100 by default
#' @param retryDelay the time in seconds to delay before retrying in case of a server error, default value of 0.3 seconds
#' @return results in a list of lists, with the scored probability at the end of each list
consumeDataframe <- function(api_key, requestURL, valuesDF, globalParam=setNames(list(), character(0)), batchSize = 250, retryDelay = 0.3) {
if (missing(api_key)) {
stop("Need to specify API key")
}
if (missing(requestURL)) {
stop("Need to specify request URL")
}
if (missing(valuesDF)) {
stop("Need to specify dataframe to be scored")
}
#format as matrix and parse column by column
columnNames = colnames(valuesDF)
matrixdf <- as.matrix(valuesDF)
rownames(matrixdf) <- NULL
colnames(matrixdf) <- NULL
matrixdf <- lapply(seq_len(nrow(matrixdf)), function(row) matrixdf[row,])
values = matrixdf
df <- data.frame(stringsAsFactors=FALSE)
valuebatch = list()
counter = 1
#process in batches and make API calls in batches
for(i in 1:(length(values))) {
valuebatch[length(valuebatch) + 1] = values[i]
if(counter == batchSize || i == (length(values))) {
temp <- callDTAPI(api_key, requestURL, columnNames, valuebatch, globalParam, retryDelay)
resultStored <- jsonlite::fromJSON(temp)
resultList = resultStored$Results$output1$value$Values
resultDF <- data.frame(resultList[,(ncol(resultList))])
#      print(resultDF)
#      print(is.data.frame(resultDF))
if(length(df) != 0 && length(resultDF) != 0) {
names(df) <- names(resultDF)
}
df <- rbind(df,resultDF)
colnames(df) <- "Scored probabilities"
#      print("passed")
print(sprintf("%i %s %i %s", i,"out of",length(values),"processed"))
valuebatch = list()
counter = 0
}
counter = counter + 1
}
colnames(df) <- "Scored probabilities"
return(df)
#   resultStored <- jsonlite::fromJSON(resultStored)
#   resultDF <- data.frame(matrix(resultStored$Results$output1$value$Values))
#   colnames(resultDF) <- resultStored$Results$output1$value$ColumnNames
#   return(resultDF)
}
#' This function is a helper that takes in an API key, values in the data table format and column names to pass to the API and the request URL (OData Endpoint Address).
#' It then obtains a response from Azure Machine Learning Studio and returns a response to the consumeFile function.
callDTAPI <- function(api_key, requestURL, columnNames, values,  globalParam, retryDelay) {
httpStatus = 0
tries = 0
# limit number of API calls to 3
for(i in 1:3) {
#make api calls and prepare payload if httpStatus indicates server error or if
if(tries == 0 || httpStatus >= 500) {
if(httpStatus >= 500) {
#delay by fixed or specified time if server error
print(paste("The request failed with status code:", httpStatus, sep=" "))
print("headers:")
print(headers)
print(sprintf("%s %f %s", "Retrying in ",retryDelay," seconds"))
Sys.sleep(retryDelay)
}
tries = tries + 1
#construct request payload and load RCurl functions
options(RCurlOptions = list(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl")))
h = RCurl::basicTextGatherer()
hdr = RCurl::basicHeaderGatherer()
req = list(
Inputs = list(
"input1" = list(
"ColumnNames" = columnNames,
"Values" = values
)
)
,GlobalParameters = globalParam
)
body = enc2utf8((rjson::toJSON(req)))
#make call to API after constructing request payload
authz_hdr = paste('Bearer', api_key, sep=' ')
h$reset()
RCurl::curlPerform(url = requestURL,
httpheader=c('Content-Type' = "application/json", 'Authorization' = authz_hdr),
postfields=body,
writefunction = h$update,
headerfunction = hdr$update,
verbose = TRUE
#                 Parameters below are needed if using test environment, but should not be included for security reasons
,ssl.verifypeer=FALSE,
ssl.verifyhost = FALSE
)
headers = hdr$value()
httpStatus = headers["status"]
result = h$value()
formatresult = result
#      formatresult <- jsonlite::toJSON(jsonlite::fromJSON(result), pretty = TRUE)
}
#return if successful
if(httpStatus == 200) {
return(formatresult)
}
#if user error, print and return error details
else if ((httpStatus>= 400) && (500 > httpStatus))
{
print(paste("The request failed with status code:", httpStatus, sep=" "))
print("headers:")
print(headers)
print(h$value())
break
}
}
return(formatresult)
}
#' This function is a helper that takes in an API key, values in the key value format and column names to pass to the API and the request URL (OData Endpoint Address).
#' It then obtains a response from Azure Machine Learning Studio and returns a response to the consumeFile function.
callAPI <- function(api_key, requestURL, keyvalues,  globalParam, retryDelay) {
httpStatus = 0
tries = 0
# limit number of API calls to 3
for(i in 1:3) {
#make api calls and prepare payload if httpStatus indicates server error or if
if(tries == 0 || httpStatus >= 500) {
if(httpStatus >= 500) {
#delay by fixed or specified time if server error
print(paste("The request failed with status code:", httpStatus, sep=" "))
print("headers:")
print(headers)
print(sprintf("%s %f %s", "Retrying in ",retryDelay," seconds"))
Sys.sleep(retryDelay)
}
tries = tries + 1
#construct request payload and load RCurl functions
options(RCurlOptions = list(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl")))
h = RCurl::basicTextGatherer()
hdr = RCurl::basicHeaderGatherer()
req = list(
Inputs = list(
input1 = keyvalues
)
,GlobalParameters = globalParam
)
body = enc2utf8((rjson::toJSON(req)))
print(body)
#make call to API after constructing request payload
authz_hdr = paste('Bearer', api_key, sep=' ')
h$reset()
RCurl::curlPerform(url = requestURL,
httpheader=c('Content-Type' = "application/json", 'Authorization' = authz_hdr),
postfields=body,
writefunction = h$update,
headerfunction = hdr$update,
verbose = TRUE
#                 Parameters below are needed if using test environment, but should not be included for security reasons
,ssl.verifypeer=FALSE,
ssl.verifyhost = FALSE
)
headers = hdr$value()
httpStatus = headers["status"]
result = h$value()
formatresult = result
}
#return if successful
if(httpStatus == 200) {
return(formatresult)
}
#if user error, print and return error details
else if ((httpStatus>= 400) && (500 > httpStatus))
{
print(paste("The request failed with status code:", httpStatus, sep=" "))
print("headers:")
print(headers)
break
}
}
return(formatresult)
}
discoverSchema <- function(wkID, token, schemes = "https", host = "requestresponse001.cloudapp.net:443", api_version = "2.0") {
# swagger document:
# schemes://hostbasepath/"swagger.json?api-version=2.0"
swaggerURL = paste(schemes,"://", host, "/workspaces/", wkID, "/services/", token,"/swagger.json?api-version=",api_version, sep = "")
httr::set_config(config(ssl_VERIFYHOST=FALSE,ssl_verifyPEER=FALSE), override=TRUE)
resp <- httr::GET(swaggerURL)
swagger <- httr::content(resp)
# condensed three steps into one line: Access JSON and then use rjson and json lite in order to structure it as a layered json object
inputschema = jsonlite::toJSON(jsonlite::fromJSON((rjson::toJSON(swagger$definitions$ExecutionInputs))), pretty = TRUE)
inputexample <- jsonlite::toJSON(jsonlite::fromJSON((rjson::toJSON(swagger$definitions$ExecutionRequest$example))), pretty = TRUE)
#find the path where operationId is execute
foundExec = FALSE
pathno = 0
foundpathindex= -1
for(execpath in swagger$paths) {
pathno = pathno + 1
for(operationpath in execpath) {
for(operation in operationpath) {
for(charac in operation) {
if(charac[1] == "execute")
{
foundExec = TRUE
foundpathindex = pathno
break
}
}
}
}
}
executepath = names(swagger$paths)[[foundpathindex]]
httpMethod = toupper(names(swagger$paths[[2]]))
# requestURL:
#   "https://requestresponse001.cloudapp.net:443/workspaces/7e8f135f31274b7eac419bd056875c03/services/a5b003e52c924d16a2e38ade45dd0154/execute?api-version=2.0&format=swagger"
#   schemes://hostbasepath(path where operationId="execute")
requestURL = paste(schemes,"://", host, "/workspaces/", wkID, "/services/", token, executepath, sep = "")
httpRequest = paste(httpMethod,requestURL)
#tell user what they can do
if(foundExec) {
consumefile = paste("consumeFile(api_key, requestURL, dataframe)")
consumedf = paste("consumeDataframe(api_key, requestURL, valuesDF)")
consumelists = paste("consumeLists(api_key, requestURL, ...)")
consumedt = paste("consumeFile(api_key, requestURL, columnNames, ...)")
cat("Sample functions to execute the web service and get a response synchronously:","\n", consumefile,"\n", consumedf,"\n", consumelists,"\n", consumedt,"\n","\n")
}
return (list("Request URL:" = requestURL, "Sample input:" = inputexample, "Input schema:" = inputschema))
}
onlineKMeans <- publishWebservice("kMeans5Clusters", "kMeans", list("age"="int", "menopause"="int", "tumor.size"="int",
"inv.nodes"="int", "node.caps"="int", "deg.malig"="int",
"breast"="int", "breast.quad"="int", "irradiat"="int"),
list("age"="int", "menopause"="int", "tumor.size"="int",
"inv.nodes"="int", "node.caps"="int", "deg.malig"="int",
"breast"="int", "breast.quad"="int", "irradiat"="int", "fit.cluster"="int"), wsID, wsAuth)
onlineKMeans <- publishWebService("kMeans5Clusters", "kMeans", list("age"="int", "menopause"="int", "tumor.size"="int",
"inv.nodes"="int", "node.caps"="int", "deg.malig"="int",
"breast"="int", "breast.quad"="int", "irradiat"="int"),
list("age"="int", "menopause"="int", "tumor.size"="int",
"inv.nodes"="int", "node.caps"="int", "deg.malig"="int",
"breast"="int", "breast.quad"="int", "irradiat"="int", "fit.cluster"="int"), wsID, wsAuth)
onlineKMeans <- publishWebService("kMeans5Clusters", "kMeansCancer", list("data.set"="string"), list("dsClusters"="string"), wsID, wsAuth)
wsID = "3612640f27234eb7b2b91ac62e8b4a40"
wsAuth = "abcbe14a958a40978f93aa0e0e71f5be"
packDependencies <- function(functionName) {
# lists for storing objects and packages
dependencies = list()
packages = list()
# generate a GUID to act as a file name to store packages, R data
guid = gsub("-", "", uuid::UUIDgenerate(use.time=TRUE))
# NOTE: will not work if the user function specifies the names directly, e.g. won't find rjson::toJSON
# from findGlobals man page: "R semantics only allow variables that might be local to be identified"
# CONSIDER: how robust is this filtering? need to verify
for (obj in codetools::findGlobals(get(functionName))) {
name = get(obj)
# filter out primitives and duplicates
if (is.primitive(name) || (obj %in% names(dependencies))) {
next
}
# get in-memory objects
# Can nonfunction objects have dependencies???
else if (!is.function(name)) {
dependencies[[obj]] <- name
nameEnv <- environment(get(class(name)))
if (!(identical(nameEnv, NULL)) && !(identical(nameEnv, .BaseNamespaceEnv))) {
packages <- recurPkg(paste(getNamespaceName(nameEnv)), packages)
}
}
# grab user defined functions
else if (identical(environment(name), globalenv())) {
dependencies[[obj]] <- name
# recursively get dependencies
results <- recurDep(obj, dependencies, packages)
dependencies <- results$dependencies
packages <- results$packages
}
# get the names of packages of package functions
# filter out base functions
else if (paste(getNamespaceName(environment(name))) != "base") {
# recursively get packages
packages <- recurPkg(paste(getNamespaceName(environment(name))), packages)
}
# need an else branch?
}
# save current path to restore to later
start = getwd()
# go to package library, doing this to prevent zipping entire package
toPack <- packages
toZip = vector()
for (i in 1:length(.libPaths())) {
setwd(.libPaths()[i])
# try to find and zip up the packages
for (pkg in toPack) {
if (file.exists(pkg)) {
zip(paste(start, paste(pkg, "zip", sep="."), sep="/"), pkg)
toZip <- c(toZip, paste(pkg, "zip", sep="."))
toPack <- toPack[toPack != pkg]
}
}
# if done packing, break
if (length(toPack) == 0) {
break
}
if (i == length(.libPaths())) {
# error: can't find packages
stop("Error: unable to locate packages. Please make sure the packages used are in at least one of the library paths.")
}
}
# go back to where the user started
setwd(start)
# objects, functions, etc.
if (length(dependencies) > 0) {
# maybe can save directly as a .zip and skip the zip() call?
save(dependencies, file=guid)
toZip <- c(toZip, guid)
}
# zip up everything
if (length(toZip) > 0) {
zip(zipfile=guid, files=toZip)
zipEnc <- base64enc::base64encode(paste(guid, ".zip", sep=""))
# delete the packages
for (pkg in packages) {
# did I miss anything? maybe extra files floating around
file.remove(paste(pkg, "zip", sep="."))
}
if (length(dependencies) > 0) {
# delete the dependency rdta file
file.remove(guid)
file.remove(paste(guid,"zip",sep="."))
}
# return the encoded zip as a string
return(list(guid, zipEnc))
}
# if nothing was zipped, return false
else {
return(list(guid, ""))
}
}
onlineKMeans <- publishWebService("kMeans5Clusters", "kMeansCancer", list("data.set"="string"), list("dsClusters"="string"), wsID, wsAuth)
endpoints <- onlineKMeans[[2]]
responseDF <- consumeDataframe(endpoints[[1]]$PrimaryKey, paste(endpoints[[1]]$ApiLocation,"/execute?api-version=2.0&details=true",sep=""), dataset)
fit
BikeShare <- read.csv(file="bikes.csv")
library(randomForest)
rf.bike <- randomForest(cnt ~ xformHr + temp +
monthCount + hum + dayWeek +
mnth + isWorking + workTime,
data = BikeShare, ntree = 500,
importance = TRUE, nodesize = 25)
importance(rf.bike)
modelFrame <- serList(list(bike.model = rf.bike))
## Extract the model from the serialized input and assign
## to a convenient name.
modelList <- unserList(modelFrame)
bike.model <- modelList$bike.model
## Output a data frame with actual and values predicted
## by the model.
library(gam)
library(randomForest)
library(kernlab)
library(nnet)
outFrame <- data.frame( actual = BikeShare$cnt,
predicted =
predict(bike.model,
newdata = BikeShare))
??serList
outFrame <- data.frame( actual = BikeShare$cnt,
predicted =
predict(rf.bike,
newdata = BikeShare))
outFrame
BikeShare <- read.csv(file="bikes.csv")
library(randomForest)
rf.bike <- randomForest(cnt ~ xformHr + temp +
monthCount + hum + dayWeek +
mnth + isWorking + workTime,
data = BikeShare, ntree = 500,
importance = TRUE, nodesize = 25)
importance(rf.bike)
## Output a data frame with actual and values predicted
## by the model.
outFrame <- data.frame( actual = BikeShare$cnt,
predicted =
predict(rf.bike,
newdata = BikeShare))
head(BikeShare)
predictBikeCount <- predict(rf.bike, newdata=data.frame("1/1/2012 12:00:00 AM", 1, 0, 0, 0, 0, -1, .5, 0, 0, 0, 0, 0, 0, 0, 0, 0))
predictBikeCount <- predict(rf.bike, newdata=data.frame("1/1/2012 12:00:00 AM", 1, 0, 0, 0, 0, -1, .5, 0, 0, 0, 0, 0, 0, 0, 5, 25, 20))
predictBikeCount <- predict(rf.bike, newdata=data.frame("1/1/2012 12:00:00 AM", 1, 0, 0, 0, 0, -1, .5, 0, 0, 0, 0, 0, 0, 0, 5, 25, "xformHR"=20))
predictBikeCount <- predict(rf.bike, newdata=data.frame("1/1/2012 12:00:00 AM", 1, 0, 0, 0, 0, -1, .5, 0, 0, 0, 0, 0, 0, 0, 5, 25, "xformHr"=20))
predictBikeCount <- function(dateTime, month, hr, hldy, wkdy, weather, temp, hum,
wndspd, casual, registered, cnt, isWorking, mthCnt, dayWeek, wrkTime, xformHr) {
return(predict(rf.bike, newdata=data.frame("dteday"=dateTime, "mnth"=month, "hr"=hr, "holiday"=hldy, "workingday"=wkdy, "weathersit"=weather,
"temp"=temp, "hum"=hum, "windspeed"=wndspd, "casual"=casual, "registered"=registered, "cnt"=cnt,
"isWorking"=isWorking, "monthCount"=mthCnt, "dayWeek"=dayWeek, "workTime"=wrkTime, "xformHr"=xformHr)))
}
predictBikeCount("1/1/2012 12:00:00 AM", 1, 0, 0, 0, 0, -1, .5, 0, 0, 0, 0, 0, 0, 0, 5, 25, "xformHr"=20)
predictBikeCount("1/1/2012 12:00:00 AM", 1, 0, 0, 0, 0, -1, .5, 0, 0, 0, 0, 0, 0, 5, 25, "xformHr"=20)
bikeCount <- publishWebService("predictBikeCount", "bikeCount", list("dteday"="date-time", "mnth"="int", "hr"="int", "holiday"="int", "workingday"="int", "weathersit"="int",
"temp"="int", "hum"="int", "windspeed"="int", "casual"="int", "registered"="int", "cnt"="int",
"isWorking"="int", "monthCount"="int", "dayWeek"="int", "workTime"="int", "xformHr"="int"),
list("count"="int"), wsID, wsAuth)
